{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVYPqyNJs7rT",
        "outputId": "8189a6c7-fee9-4bbb-e138-a05938438989"
      },
      "source": [
        "!pip3 install scikit-learn==0.21.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/a4/a48bd4b0d15395362b561df7e7247de87291105eb736a3b2aaffebf437b9/scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2) (1.0.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.21.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng8x6RnrrKpn",
        "outputId": "1988c343-5a22-4414-9a5e-7f798f5f4919"
      },
      "source": [
        "#connection drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip3 install TurkishStemmer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting TurkishStemmer\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/bf/3e56dd4ce442f9237e1c202ce736ae5e5818d74f81604f1665e67736cfc0/TurkishStemmer-1.3-py3-none-any.whl\n",
            "Installing collected packages: TurkishStemmer\n",
            "Successfully installed TurkishStemmer-1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06WfFrgOU7k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0d5338-3c29-4a0b-c5b2-a12922a74a1a"
      },
      "source": [
        "%tensorflow_version 2.4\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.4`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Tensorflow version 2.4.1\n",
            "Running on TPU  ['10.3.130.18:8470']\n",
            "WARNING:tensorflow:TPU system grpc://10.3.130.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.3.130.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.3.130.18:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.3.130.18:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oWaUc5nsugk"
      },
      "source": [
        "from myfiles.utils import utils"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh-5XBNk04H2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bSAjgK8x7jY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643e6676-efed-47e8-b5c1-49e0c6f8e50e"
      },
      "source": [
        "X,y = utils.getXyData()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "init\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dmsjsW_QBn"
      },
      "source": [
        "def getEmbeddings(word_index):\n",
        "    \n",
        "  glove_dir = '/content/drive/MyDrive/lstmWorks/vectors.txt'\n",
        "  word2vec_dir = '/content/drive/MyDrive/Word2Vec/trmodel'\n",
        "\n",
        "  gloveEmbedMatrix = utils.makeEmbeddingMatrix(300,'glove', glove_dir ,word_index )\n",
        "  word2vecEmbedMatrix = utils.makeEmbeddingMatrix(400, 'word2vec', word2vec_dir, word_index)\n",
        "  return gloveEmbedMatrix, word2vecEmbedMatrix\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8AtaNeO_QOC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbrn4qZDfiBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d246e2-9f70-4485-c0b1-06ea2d138de9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "# import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "!pip3 install h5py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfJkAxHwHHHZ"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "import numpy as np \n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "def createTokenizer(trainSequences, testSequences, y_train, y_test):\n",
        "    maxlen = 100     # max number of words in a comment to use\n",
        "    tokenizer = Tokenizer(num_words = 25000)   #most max words\n",
        "    tokenizer.fit_on_texts(trainSequences)   #kelimeleri fit'liyoruz.\n",
        "\n",
        "    x_train = tokenizer.texts_to_sequences(trainSequences)\n",
        "    x_test = tokenizer.texts_to_sequences(testSequences)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "    x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    y_train = np.asarray(y_train)\n",
        "    y_test = np.asarray(y_test)\n",
        "\n",
        "    print('Shape of data tensor:', y_train.shape)\n",
        "    print('Shape of label tensor:', y_test.shape) \n",
        "    \n",
        "    # return [x_train, y_train], word_index\n",
        "    return [x_train, x_test,y_train, y_test], word_index\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRtKhqDjQZXf"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAzOxu9Nobqg"
      },
      "source": [
        "def makeNewModelWithCheckPoint(lstmLayer,denseLayer,embedding_matrix,total_words):\n",
        "\n",
        "    # from keras.models import Sequential\n",
        "    # from keras.layers import LSTM, Dense, Dropout, Masking, Embedding,Bidirectional\n",
        "    # from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "    # from keras.utils.vis_utils import plot_model\n",
        "\n",
        "    import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "\n",
        "    from keras.preprocessing.text import Tokenizer\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "    from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "    from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "    from keras.models import Model\n",
        "    from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "    # glove_matrix , word2vec_matrix  = getEmbeddings(word_index)\n",
        "\n",
        "\n",
        "    inp = Input(shape=(100,))\n",
        "    if embedding_matrix.shape[1] == 300:\n",
        "        x = Embedding(25000, 300, weights=[embedding_matrix])(inp)\n",
        "    elif embedding_matrix.shape[1] == 400:\n",
        "        x = Embedding(25000, 400, weights=[embedding_matrix])(inp)\n",
        "\n",
        "\n",
        "    x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
        "\n",
        "    x = Dense(100, activation=\"relu\")(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.summary()\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "    return model "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt9LXbhRg-qX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81ea5htIJV0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bdedbf-905f-48e0-dd30-b1700f332af4"
      },
      "source": [
        "# gridSearch works...\n",
        "maxlen=100\n",
        "a = utils.splitKFoldStratified(X,y)\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    for i in range(5):\n",
        "        x_train = a[0][i]\n",
        "        x_test = a[1][i]\n",
        "        y_train = a[2][i]\n",
        "        y_test = a[3][i]\n",
        "\n",
        "        element, word_index = createTokenizer(x_train, x_test, y_train, y_test)\n",
        "        \n",
        "        sendParam = train_test_split(element[0], element[2], test_size=0.30, random_state=42, stratify=element[2] ) \n",
        "\n",
        "        \n",
        "        # element, word_index = createTokenizer(X, x_test, y, y_test)\n",
        "        glove_matrix , word2vec_matrix  = getEmbeddings(word_index)\n",
        "        total_words = len(word_index) + 1\n",
        "        print(total_words) \n",
        "        model = makeNewModelWithCheckPoint(1,1,word2vec_matrix,total_words)\n",
        "        history = model.fit(sendParam[0], sendParam[2], batch_size=128, epochs=10, validation_data=(sendParam[1],sendParam[3])) \n",
        "\n",
        "        print(model.evaluate(element[1], element[3]))\n",
        "        utils.vizualize_loss_acc(history)\n",
        "        # utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "# xtrain,xtest,ytrain,ytest\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">Train: 0=11903, 1=175795, Test: 0=2976, 1=43949\n",
            ">Train: 0=11903, 1=175795, Test: 0=2976, 1=43949\n",
            ">Train: 0=11903, 1=175795, Test: 0=2976, 1=43949\n",
            ">Train: 0=11903, 1=175795, Test: 0=2976, 1=43949\n",
            ">Train: 0=11904, 1=175796, Test: 0=2975, 1=43948\n",
            "Found 89942 unique tokens.\n",
            "Shape of data tensor: (187698,)\n",
            "Shape of label tensor: (46925,)\n",
            "Embeddings Matrix shape :  (25000, 300)\n",
            "Embeddings Matrix shape :  (25000, 400)\n",
            "89943\n",
            "Model: \"model_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_31 (InputLayer)        [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_30 (Embedding)     (None, 100, 400)          10000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_29 (Bidirectio (None, 100, 200)          400800    \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 100, 100)          20100     \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 100, 1)            101       \n",
            "=================================================================\n",
            "Total params: 10,421,001\n",
            "Trainable params: 10,421,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1027/1027 [==============================] - 67s 54ms/step - loss: 0.2329 - acc: 0.9339 - val_loss: 0.1603 - val_acc: 0.9388\n",
            "Epoch 2/10\n",
            "1027/1027 [==============================] - 47s 46ms/step - loss: 0.1520 - acc: 0.9412 - val_loss: 0.1528 - val_acc: 0.9446\n",
            "Epoch 3/10\n",
            "1027/1027 [==============================] - 47s 46ms/step - loss: 0.1226 - acc: 0.9546 - val_loss: 0.1465 - val_acc: 0.9498\n",
            "Epoch 4/10\n",
            "1027/1027 [==============================] - 47s 46ms/step - loss: 0.0973 - acc: 0.9658 - val_loss: 0.1559 - val_acc: 0.9470\n",
            "Epoch 5/10\n",
            "1026/1027 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9721"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEDaX7RxJV3n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2bw4h8jGLuY"
      },
      "source": [
        "\n",
        "# with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    # param_grid = dict( lstmLayer = [10,32,64,128],\n",
        "    #                     denseLayer=[32,64],\n",
        "    #                     total_words=[total_words],\n",
        "    #                     embedding_matrix=[glove_matrix])\n",
        "\n",
        "    # model = KerasClassifier(build_fn = makeNewModelWithCheckPoint,\n",
        "    #                         epochs=10, batch_size=256,\n",
        "    #                         verbose=True)\n",
        "\n",
        "    # cross_val = StratifiedKFold(n_splits=5)\n",
        "    \n",
        "\n",
        "    # grid = GridSearchCV(\n",
        "    #                       estimator=model, param_grid=param_grid,\n",
        "    #                       return_train_score=True,\n",
        "    #                       n_jobs=1, \n",
        "    #                       scoring=['accuracy'],\n",
        "    #                      refit='accuracy', cv=cross_val\n",
        "    #                     )\n",
        "    \n",
        "    # grid.fit(element[0], element[1])\n",
        "    # # summarize results\n",
        "    # print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
        "    # print('***************************')\n",
        "\n",
        "    # # print('Test Score for Optimized Parameters:', grid.score(x_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybadh73UgQLf"
      },
      "source": [
        "!pip3 install tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "import os\n",
        "resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
        "if resp.status_code != 200:\n",
        "  print(\"Failed to switch the TPU to TF {}\".format(version))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H39iB3a_GLyM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWiHKC2MGL1G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzSY90FEGL4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av5nOLRrGMWv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKQLSLlWJV6a"
      },
      "source": [
        "# gridSearch works...\n",
        "# maxlen=100\n",
        "# a = utils.splitKFoldStratified(X,y)\n",
        "\n",
        "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "# # for i in range(5):\n",
        "# x_train = a[0][0]\n",
        "# x_test = a[1][0]\n",
        "# y_train = a[2][0]\n",
        "# y_test = a[3][0]\n",
        "\n",
        "# element, word_index = createTokenizer(X, x_test, y, y_test)\n",
        "\n",
        "# total_words = len(word_index) + 1\n",
        "# print(total_words) \n",
        "# # xtrain,xtest,ytrain,ytest\n",
        "# # sendParam = train_test_split(element[0], element[2], test_size=0.20, random_state=42, stratify=element[2] )\n",
        "\n",
        "# with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "\n",
        "#     # Parameter grid for grid search\n",
        "#     param_grid = dict( lstmLayer = [32,64,128],\n",
        "#                         denseLayer=[32,64],\n",
        "#                         total_words=[total_words],\n",
        "#                         embedding_matrix=[glove_matrix])\n",
        "\n",
        "#     model = KerasClassifier(build_fn = makeNewModelWithCheckPoint,\n",
        "#                             epochs=10, batch_size=256,\n",
        "#                             verbose=True)\n",
        "\n",
        "#     cross_val = StratifiedKFold(n_splits=5)\n",
        "    \n",
        "\n",
        "#     grid = GridSearchCV(\n",
        "#                           estimator=model, param_grid=param_grid,\n",
        "#                           return_train_score=True,\n",
        "#                           n_jobs=1, \n",
        "#                           scoring=['accuracy'],\n",
        "#                          refit='accuracy', cv=cross_val\n",
        "#                         )\n",
        "    \n",
        "#     grid.fit(element[0], element[1])\n",
        "#     # summarize results\n",
        "#     print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
        "#     print('***************************')\n",
        "\n",
        "#     # print('Test Score for Optimized Parameters:', grid.score(x_test, y_test))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WNw2gfRJV9P"
      },
      "source": [
        "# en uygun modeli bulmak için uyguladığım fonk.\n",
        "# def _makeNewModelWithCheckPoint(lstmLayer,denseLayer,embedding_matrix,total_words):\n",
        "\n",
        "#     from keras.models import Sequential\n",
        "#     from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
        "#     from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "#     from keras.utils.vis_utils import plot_model\n",
        "#     # glove_matrix , word2vec_matrix  = getEmbeddings(word_index)\n",
        "\n",
        "\n",
        "#     model = Sequential()\n",
        "#     model.add(Embedding(input_dim = total_words, # number of input  \n",
        "#                         input_length=100, \n",
        "#                         weights=[embedding_matrix],\n",
        "#                         output_dim=300 , trainable=False, mask_zero=True))\n",
        "#     # Masking layer for pre-trained embeddings\n",
        "#     model.add(Masking(mask_value=0.0))\n",
        "\n",
        "\n",
        "#     # Recurrent layer\n",
        "#     model.add(LSTM(lstmLayer, return_sequences=False, \n",
        "#                    dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "#     # Fully connected layer\n",
        "#     model.add(Dense(denseLayer, activation='relu'))  #32 neurons\n",
        "\n",
        "#     # Dropout for regularization\n",
        "#     model.add(Dropout(0.25))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     import keras\n",
        "#     model.summary()\n",
        "\n",
        "\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # simple early stopping\n",
        "#     # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "#     # mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "#     return model \n",
        "\n",
        "\n",
        "#     # history = model.fit(param[0], param[1],\n",
        "#     #                         epochs=20,\n",
        "#     #                         validation_data=(val[0],val[1]),\n",
        "#     #                         batch_size=512,\n",
        "#     #                         verbose=1, \n",
        "#     #                         callbacks=[es, mc])\n",
        "\n",
        "#     # print(\"*********************test***********************\")\n",
        "#     # print(model.evaluate(x_test, y_test))\n",
        "#     # print(\"---------------------vizualize---------------------\")\n",
        "#     # vizualize_loss_acc(history)\n",
        "#     # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrNLTKyeJWAD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBieFGEGJWCm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzLt5e9ZJWE2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKmsZriwJWHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRSrqR1dq9Aw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9B1veVCq9Aw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}